{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6\n",
    "## Akarsh Sahu\n",
    "### 11-6-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Import the data: You are provided separate .csv files for train and test.\n",
    "\n",
    "1. Train shape: (507, 148)\n",
    "2. Test shape: (168, 148)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(507, 148)\n",
      "(168, 148)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Remove any rows that have missing data across both sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()\n",
    "train = train.dropna(how='any',axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()\n",
    "test = test.dropna(how='any',axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. The target variable (dependent variable) is called \"class\", make sure to separate this out into a \"y_train\" and \"y_test\" and remove from your \"X_train\" and \"X_test\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[['class']]\n",
    "X_train = train.drop(['class'], axis = 1)\n",
    "\n",
    "y_test = test[['class']]\n",
    "X_test = test.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Scale all features / predictors (NOT THE TARGET VARIABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-6fa0c0a574fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_test_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_train_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_t = scaler.transform(X_train)\n",
    "X_test_t = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest Classifier - Base Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Use the RandomForestClassifier in sklearn. Fit your model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl1_rf = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=0)\n",
    "mdl1_rf.fit(X_train_t, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Use the fitted model to predict on test data. Use the .predict() method to get the predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl1_pred = mdl1_rf.predict(X_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Calculate the confusion matrix and classification report for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[14  0  0  0  0  0  0  0  0]\n",
      " [ 1 21  0  2  1  0  0  0  0]\n",
      " [ 1  3 10  1  0  0  0  0  0]\n",
      " [ 0  4  0 19  0  0  0  0  0]\n",
      " [ 0  0  0  0 24  0  0  0  5]\n",
      " [ 1  1  1  0  0 12  0  0  0]\n",
      " [ 3  0  0  0  0  0 13  0  0]\n",
      " [ 0  1  0 10  3  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  0  0 15]]\n",
      "\n",
      "\n",
      "\n",
      "Classification Report\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.70      1.00      0.82        14\n",
      "   building        0.70      0.84      0.76        25\n",
      "        car        0.91      0.67      0.77        15\n",
      "   concrete        0.58      0.83      0.68        23\n",
      "      grass        0.83      0.83      0.83        29\n",
      "       pool        1.00      0.80      0.89        15\n",
      "     shadow        1.00      0.81      0.90        16\n",
      "       soil        0.00      0.00      0.00        14\n",
      "       tree        0.75      0.88      0.81        17\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       168\n",
      "   macro avg       0.72      0.74      0.72       168\n",
      "weighted avg       0.73      0.76      0.73       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "\n",
    "#Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, mdl1_pred)\n",
    "print(\"Confusion Matrix\\n\\n\", cnf_matrix)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#Classification Report\n",
    "cl_report = metrics.classification_report(y_test, mdl1_pred)\n",
    "print(\"Classification Report\\n\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl1_pred_tr = mdl1_rf.predict(X_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[43  0  0  0  1  0  1  0  0]\n",
      " [ 2 87  1  7  0  0  0  0  0]\n",
      " [ 0  0 20  1  0  0  0  0  0]\n",
      " [ 0  4  0 88  0  0  0  0  1]\n",
      " [ 0  1  0  0 76  0  0  0  6]\n",
      " [ 0  1  0  0  1 12  0  0  0]\n",
      " [ 2  0  0  0  0  0 42  0  1]\n",
      " [ 0  3  0  5  1  0  0 11  0]\n",
      " [ 1  0  0  0  5  0  1  0 82]]\n",
      "\n",
      "\n",
      "\n",
      "Classification Report\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.90      0.96      0.92        45\n",
      "   building        0.91      0.90      0.90        97\n",
      "        car        0.95      0.95      0.95        21\n",
      "   concrete        0.87      0.95      0.91        93\n",
      "      grass        0.90      0.92      0.91        83\n",
      "       pool        1.00      0.86      0.92        14\n",
      "     shadow        0.95      0.93      0.94        45\n",
      "       soil        1.00      0.55      0.71        20\n",
      "       tree        0.91      0.92      0.92        89\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       507\n",
      "   macro avg       0.93      0.88      0.90       507\n",
      "weighted avg       0.91      0.91      0.91       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_train, mdl1_pred_tr)\n",
    "print(\"Confusion Matrix\\n\\n\", cnf_matrix)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#Classification Report\n",
    "cl_report = metrics.classification_report(y_train, mdl1_pred_tr)\n",
    "print(\"Classification Report\\n\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are clear signs of overfitting as the metrics(precision, recall, f1-score, accuracy) for the training dataset are significantly higher as compared to the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Identify the top 5 features. Feel free to print a list OR to make a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_importances = pd.Series(mdl1_rf.feature_importances_, index=X_train.columns)\n",
    "plt = feat_importances.nlargest(5).plot(kind='barh')\n",
    "print(plt.invert_yaxis())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LinearSVM Classifier - Base Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a simple LinearSVC Classifier only using default parameters.\n",
    "\n",
    "#### a. Use the LinearSVC in sklearn. Fit your model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "mdl2_lsvc = LinearSVC()\n",
    "mdl2_lsvc.fit(X_train_t, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Use the fitted model to predict on test data. Use the .predict() method to get the predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl2_pred = mdl2_lsvc.predict(X_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Calculate the confusion matrix and classification report for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 22  1  1  1  0  0  0  0]\n",
      " [ 0  2 12  0  0  0  0  0  1]\n",
      " [ 1  6  0 15  0  0  0  0  1]\n",
      " [ 0  0  0  1 26  0  0  0  2]\n",
      " [ 1  0  1  0  0 13  0  0  0]\n",
      " [ 2  0  0  0  0  0 14  0  0]\n",
      " [ 0  4  0  1  3  0  0  6  0]\n",
      " [ 0  0  0  1  7  0  0  0  9]]\n",
      "\n",
      "\n",
      "\n",
      "Classification Report\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.76      0.93      0.84        14\n",
      "   building        0.65      0.88      0.75        25\n",
      "        car        0.86      0.80      0.83        15\n",
      "   concrete        0.79      0.65      0.71        23\n",
      "      grass        0.70      0.90      0.79        29\n",
      "       pool        1.00      0.87      0.93        15\n",
      "     shadow        0.93      0.88      0.90        16\n",
      "       soil        1.00      0.43      0.60        14\n",
      "       tree        0.69      0.53      0.60        17\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       168\n",
      "   macro avg       0.82      0.76      0.77       168\n",
      "weighted avg       0.80      0.77      0.77       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, mdl2_pred)\n",
    "print(\"Confusion Matrix\\n\\n\", cnf_matrix)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#Classification Report\n",
    "cl_report = metrics.classification_report(y_test, mdl2_pred)\n",
    "print(\"Classification Report\\n\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl2_pred_tr = mdl2_lsvc.predict(X_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[45  0  0  0  0  0  0  0  0]\n",
      " [ 0 97  0  0  0  0  0  0  0]\n",
      " [ 0  0 21  0  0  0  0  0  0]\n",
      " [ 0  0  0 93  0  0  0  0  0]\n",
      " [ 0  1  0  0 80  0  0  0  2]\n",
      " [ 0  0  0  0  0 14  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  0]\n",
      " [ 0  0  0  0  0  0  0  0 89]]\n",
      "\n",
      "\n",
      "\n",
      "Classification Report\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        1.00      1.00      1.00        45\n",
      "   building        0.99      1.00      0.99        97\n",
      "        car        1.00      1.00      1.00        21\n",
      "   concrete        1.00      1.00      1.00        93\n",
      "      grass        1.00      0.96      0.98        83\n",
      "       pool        1.00      1.00      1.00        14\n",
      "     shadow        1.00      1.00      1.00        45\n",
      "       soil        1.00      1.00      1.00        20\n",
      "       tree        0.98      1.00      0.99        89\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       507\n",
      "   macro avg       1.00      1.00      1.00       507\n",
      "weighted avg       0.99      0.99      0.99       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_train, mdl2_pred_tr)\n",
    "print(\"Confusion Matrix\\n\\n\", cnf_matrix)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#Classification Report\n",
    "cl_report = metrics.classification_report(y_train, mdl2_pred_tr)\n",
    "print(\"Classification Report\\n\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are clear signs of overfitting as the metrics(precision, recall, f1-score, accuracy) for the training dataset are significantly higher as compared to the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machine Classifier + Linear Kernel + Grid Search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Use SVC from sklearn with kernel = \"linear\". Run the GridSearchCV using the following (SVMs run much faster than RandomForest):\n",
    "\n",
    "C: 0.01 - 10 in increments of 0.2 (consider using the np.arange() method from numpy to build out a sequence of values)\n",
    "\n",
    "Note: Feel free to try out more parameters, the above is the bare minimum for this assignment.\n",
    "\n",
    "Use 5 cross-fold and the default scoring. Please set verbose = 0 to reduce the printing (sorry to our grader for not specifying this last week!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "sv_obj = SVC(kernel = 'linear')\n",
    "\n",
    "param_grid = {'C': np.arange(0.01, 10, 0.2).tolist()}\n",
    "\n",
    "svc_Grid = GridSearchCV(sv_obj, param_grid, cv=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.01, 0.21000000000000002, 0.41000000000000003, 0.6100000000000001, 0.81, 1.01, 1.2100000000000002, 1.4100000000000001, 1.61, 1.81, 2.01, 2.21, 2.41, 2.61, 2.81, 3.01, 3.21, 3.41, 3.61, 3.81, 4.01, 4.21, 4.41, 4.61, 4.8100000000000005, 5.01, 5.21, 5.41, 5.61, 5.8100000000000005, 6.01, 6.21, 6.41, 6.61, 6.8100000000000005, 7.01, 7.21, 7.41, 7.61, 7.8100000000000005, 8.01, 8.21, 8.41, 8.61, 8.81, 9.01, 9.21, 9.41, 9.610000000000001, 9.81]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_Grid.fit(X_train_t, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Identify the best performing model:\n",
    "\n",
    "1. .best_params_() : This method outputs to best performing parameters\n",
    "2. .best_estimator_() : This method outputs the best performing model, and can be used for predicting on the X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl3_svc = svc_Grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl3_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Use the best estimator model to predict on test data. Use the .predict() method to get the predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl3_pred = mdl3_svc.predict(X_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Calculate the confusion matrix and classification report for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 22  0  2  1  0  0  0  0]\n",
      " [ 0  1 14  0  0  0  0  0  0]\n",
      " [ 0  5  0 17  0  0  0  1  0]\n",
      " [ 0  0  0  1 25  0  0  0  3]\n",
      " [ 0  0  0  0  0 14  1  0  0]\n",
      " [ 1  0  0  0  0  0 15  0  0]\n",
      " [ 0  3  0  5  2  0  0  4  0]\n",
      " [ 0  0  0  1  2  0  0  0 14]]\n",
      "\n",
      "\n",
      "\n",
      "Classification Report\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.93      0.93      0.93        14\n",
      "   building        0.71      0.88      0.79        25\n",
      "        car        1.00      0.93      0.97        15\n",
      "   concrete        0.65      0.74      0.69        23\n",
      "      grass        0.83      0.86      0.85        29\n",
      "       pool        1.00      0.93      0.97        15\n",
      "     shadow        0.88      0.94      0.91        16\n",
      "       soil        0.80      0.29      0.42        14\n",
      "       tree        0.82      0.82      0.82        17\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       168\n",
      "   macro avg       0.85      0.81      0.82       168\n",
      "weighted avg       0.83      0.82      0.81       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, mdl3_pred)\n",
    "print(\"Confusion Matrix\\n\\n\", cnf_matrix)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#Classification Report\n",
    "cl_report = metrics.classification_report(y_test, mdl3_pred)\n",
    "print(\"Classification Report\\n\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e.  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl3_pred_tr = mdl3_svc.predict(X_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[40  0  0  0  0  0  5  0  0]\n",
      " [ 2 87  0  7  0  0  1  0  0]\n",
      " [ 0  1 19  1  0  0  0  0  0]\n",
      " [ 0  9  0 83  1  0  0  0  0]\n",
      " [ 0  1  0  0 70  0  0  0 12]\n",
      " [ 0  1  0  0  1 12  0  0  0]\n",
      " [ 1  0  0  0  0  0 43  0  1]\n",
      " [ 0  3  0  4  2  0  0 11  0]\n",
      " [ 0  0  0  0  3  0  1  0 85]]\n",
      "\n",
      "\n",
      "\n",
      "Classification Report\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.93      0.89      0.91        45\n",
      "   building        0.85      0.90      0.87        97\n",
      "        car        1.00      0.90      0.95        21\n",
      "   concrete        0.87      0.89      0.88        93\n",
      "      grass        0.91      0.84      0.88        83\n",
      "       pool        1.00      0.86      0.92        14\n",
      "     shadow        0.86      0.96      0.91        45\n",
      "       soil        1.00      0.55      0.71        20\n",
      "       tree        0.87      0.96      0.91        89\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       507\n",
      "   macro avg       0.92      0.86      0.88       507\n",
      "weighted avg       0.89      0.89      0.89       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_train, mdl3_pred_tr)\n",
    "print(\"Confusion Matrix\\n\\n\", cnf_matrix)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#Classification Report\n",
    "cl_report = metrics.classification_report(y_train, mdl3_pred_tr)\n",
    "print(\"Classification Report\\n\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are no signs of overfitting as the metrics(precision, recall, f1-score, accuracy) for the training dataset are comparable to that of test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Support Vector Machine Classifier + Polynomial Kernel + Grid Search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Use SVC from sklearn with kernel = \"poly\". Run the GridSearchCV using the following:\n",
    "\n",
    "1. C: 0.01 - 10 in increments of 0.2\n",
    "2. degree: 2, 3, 4, 5, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "sv_obj = SVC(kernel = 'poly')\n",
    "\n",
    "param_grid = {'C': np.arange(0.01, 10, 0.2).tolist(),\n",
    "             'degree': [2,3,4,5,6]}\n",
    "\n",
    "svc_Grid = GridSearchCV(sv_obj, param_grid, cv=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.01, 0.21000000000000002, 0.41000000000000003, 0.6100000000000001, 0.81, 1.01, 1.2100000000000002, 1.4100000000000001, 1.61, 1.81, 2.01, 2.21, 2.41, 2.61, 2.81, 3.01, 3.21, 3.41, 3.61, 3.81, 4.01, 4.21, 4.41, 4.61, 4.8100000000000005, 5.01, 5.21, 5.41, 5.61, 5.8100000000000005, 6.01, 6.21, 6.41, 6.61, 6.8100000000000005, 7.01, 7.21, 7.41, 7.61, 7.8100000000000005, 8.01, 8.21, 8.41, 8.61, 8.81, 9.01, 9.21, 9.41, 9.610000000000001, 9.81], 'degree': [2, 3, 4, 5, 6]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_Grid.fit(X_train_t, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Identify the best performing model:\n",
    "\n",
    "1. .best_params_() : This method outputs to best performing parameters\n",
    "2. .best_estimator_() : This method outputs the best performing model, and can be used for predicting on the X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 8.41, 'degree': 3}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=4.21, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl4_svc = svc_Grid.best_estimator_\n",
    "mdl4_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Use the best estimator model to predict on test data. Use the .predict() method to get the predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl4_pred = mdl4_svc.predict(X_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Calculate the confusion matrix and classification report for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 22  0  2  1  0  0  0  0]\n",
      " [ 0  2 11  0  0  1  0  1  0]\n",
      " [ 0  5  0 17  0  0  0  1  0]\n",
      " [ 0  0  0  0 26  0  0  1  2]\n",
      " [ 0  0  0  0  0 14  1  0  0]\n",
      " [ 2  0  0  0  0  0 14  0  0]\n",
      " [ 0  3  0  4  6  0  0  1  0]\n",
      " [ 0  0  0  1  3  0  0  0 13]]\n",
      "\n",
      "\n",
      "\n",
      "Classification Report\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.87      0.93      0.90        14\n",
      "   building        0.69      0.88      0.77        25\n",
      "        car        1.00      0.73      0.85        15\n",
      "   concrete        0.71      0.74      0.72        23\n",
      "      grass        0.72      0.90      0.80        29\n",
      "       pool        0.93      0.93      0.93        15\n",
      "     shadow        0.88      0.88      0.88        16\n",
      "       soil        0.25      0.07      0.11        14\n",
      "       tree        0.87      0.76      0.81        17\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       168\n",
      "   macro avg       0.77      0.76      0.75       168\n",
      "weighted avg       0.76      0.78      0.76       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, mdl4_pred)\n",
    "print(\"Confusion Matrix\\n\\n\", cnf_matrix)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#Classification Report\n",
    "cl_report = metrics.classification_report(y_test, mdl4_pred)\n",
    "print(\"Classification Report\\n\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e.  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl4_pred_tr = mdl4_svc.predict(X_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[45  0  0  0  0  0  0  0  0]\n",
      " [ 0 97  0  0  0  0  0  0  0]\n",
      " [ 0  0 20  0  1  0  0  0  0]\n",
      " [ 0  1  0 91  1  0  0  0  0]\n",
      " [ 0  1  0  0 82  0  0  0  0]\n",
      " [ 0  0  0  0  1 13  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0]\n",
      " [ 0  0  0  0  5  0  0 15  0]\n",
      " [ 0  0  0  0  1  0  0  0 88]]\n",
      "\n",
      "\n",
      "\n",
      "Classification Report\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        1.00      1.00      1.00        45\n",
      "   building        0.98      1.00      0.99        97\n",
      "        car        1.00      0.95      0.98        21\n",
      "   concrete        1.00      0.98      0.99        93\n",
      "      grass        0.90      0.99      0.94        83\n",
      "       pool        1.00      0.93      0.96        14\n",
      "     shadow        1.00      1.00      1.00        45\n",
      "       soil        1.00      0.75      0.86        20\n",
      "       tree        1.00      0.99      0.99        89\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       507\n",
      "   macro avg       0.99      0.95      0.97       507\n",
      "weighted avg       0.98      0.98      0.98       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_train, mdl4_pred_tr)\n",
    "print(\"Confusion Matrix\\n\\n\", cnf_matrix)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#Classification Report\n",
    "cl_report = metrics.classification_report(y_train, mdl4_pred_tr)\n",
    "print(\"Classification Report\\n\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are clear signs of overfitting as the metrics(precision, recall, f1-score, accuracy) for the training dataset are significantly higher as compared to the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Support Vector Machine Classifier + RBF Kernel + Grid Search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Use SVC from sklearn with kernel = \"rbf\". Run the GridSearchCV using the following:\n",
    "\n",
    "1. C: 0.01 - 10 in increments of 0.2\n",
    "2. gamma: 0.01,  0.1, 1, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "sv_obj = SVC(kernel = 'rbf')\n",
    "\n",
    "param_grid = {'C': np.arange(0.01, 10, 0.2).tolist(),\n",
    "             'gamma': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "svc_Grid = GridSearchCV(sv_obj, param_grid, cv=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.01, 0.21000000000000002, 0.41000000000000003, 0.6100000000000001, 0.81, 1.01, 1.2100000000000002, 1.4100000000000001, 1.61, 1.81, 2.01, 2.21, 2.41, 2.61, 2.81, 3.01, 3.21, 3.41, 3.61, 3.81, 4.01, 4.21, 4.41, 4.61, 4.8100000000000005, 5.01, 5.21, 5.41, 5.61, 5.8100000000000005, 6.01, 6.21, 6.41, 6.61, 6.8100000000000005, 7.01, 7.21, 7.41, 7.61, 7.8100000000000005, 8.01, 8.21, 8.41, 8.61, 8.81, 9.01, 9.21, 9.41, 9.610000000000001, 9.81], 'gamma': [0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_Grid.fit(X_train_t, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Identify the best performing model:\n",
    "\n",
    "1. .best_params_() : This method outputs to best performing parameters\n",
    "2. .best_estimator_() : This method outputs the best performing model, and can be used for predicting on the X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 4.21, 'gamma': 0.01}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=4.21, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl5_svc = svc_Grid.best_estimator_\n",
    "mdl5_svc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Use the best estimator model to predict on test data. Use the .predict() method to get the predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl5_pred = mdl5_svc.predict(X_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Calculate the confusion matrix and classification report for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 21  0  3  1  0  0  0  0]\n",
      " [ 0  1 13  1  0  0  0  0  0]\n",
      " [ 0  4  0 19  0  0  0  0  0]\n",
      " [ 0  1  0  0 26  0  0  0  2]\n",
      " [ 0  0  0  0  0 14  1  0  0]\n",
      " [ 1  0  0  0  0  0 15  0  0]\n",
      " [ 0  2  0  4  3  0  0  5  0]\n",
      " [ 0  0  0  1  1  0  0  0 15]]\n",
      "\n",
      "\n",
      "\n",
      "Classification Report\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.93      0.93      0.93        14\n",
      "   building        0.72      0.84      0.78        25\n",
      "        car        1.00      0.87      0.93        15\n",
      "   concrete        0.68      0.83      0.75        23\n",
      "      grass        0.84      0.90      0.87        29\n",
      "       pool        1.00      0.93      0.97        15\n",
      "     shadow        0.88      0.94      0.91        16\n",
      "       soil        1.00      0.36      0.53        14\n",
      "       tree        0.88      0.88      0.88        17\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       168\n",
      "   macro avg       0.88      0.83      0.84       168\n",
      "weighted avg       0.86      0.84      0.83       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, mdl5_pred)\n",
    "print(\"Confusion Matrix\\n\\n\", cnf_matrix)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#Classification Report\n",
    "cl_report = metrics.classification_report(y_test, mdl5_pred)\n",
    "print(\"Classification Report\\n\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e.  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl5_pred_tr = mdl5_svc.predict(X_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[45  0  0  0  0  0  0  0  0]\n",
      " [ 0 97  0  0  0  0  0  0  0]\n",
      " [ 0  0 21  0  0  0  0  0  0]\n",
      " [ 0  1  0 92  0  0  0  0  0]\n",
      " [ 0  1  0  0 82  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  0]\n",
      " [ 0  0  0  0  1  0  0  0 88]]\n",
      "\n",
      "\n",
      "\n",
      "Classification Report\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        1.00      1.00      1.00        45\n",
      "   building        0.98      1.00      0.99        97\n",
      "        car        1.00      1.00      1.00        21\n",
      "   concrete        1.00      0.99      0.99        93\n",
      "      grass        0.99      0.99      0.99        83\n",
      "       pool        1.00      1.00      1.00        14\n",
      "     shadow        1.00      1.00      1.00        45\n",
      "       soil        1.00      1.00      1.00        20\n",
      "       tree        1.00      0.99      0.99        89\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       507\n",
      "   macro avg       1.00      1.00      1.00       507\n",
      "weighted avg       0.99      0.99      0.99       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_train, mdl5_pred_tr)\n",
    "print(\"Confusion Matrix\\n\\n\", cnf_matrix)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#Classification Report\n",
    "cl_report = metrics.classification_report(y_train, mdl5_pred_tr)\n",
    "print(\"Classification Report\\n\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yes, similar to the model above there are signs of overfitting. The classification report for train data has significantly higher numbers compared to the ones from the test data. For example, the accuracy is a lot higher for train than test (0.99 vs 0.83), and similar story for precision, recall and f1-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conceptual Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) From the models run in steps 2-6, which performs the best based on the Classification Report? Support your reasoning with evidence around your test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the scores from the classification reports, model 2, 3, 5 and 6 had extremely good scores. For example, the accuracy scores for Support Vector Machine Classifier + RBF Kernel had 0.83 for test and 0.99 for train. But if we define the performance of a model based on how well the model generalizes, model 4 (Support Vector Machine Classifier + Linear Kernel + Grid Search) performed the best because even though there were some signs of overfitting, they were not as bad compared to other models. i.e. The classification report for train data slightly higher numbers compared to the ones from the test data. For example, the accuracy is a little higher for train than test (0.89 vs 0.81), and mostly similar story for precision, recall and f1-scores (values were not significantly different). The model did a pretty good job generalizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Compare models run for steps 4-6 where different kernels were used. What is the benefit of using a polynomial or rbf kernel over a linear kernel? What could be a downside of using a polynomial or rbf kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4 - SVM + Linear Kernel\n",
    "\n",
    "- Best parameters: SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
    "  shrinking=True, tol=0.001, verbose=False)\n",
    "- Accuracy for test: 0.81\n",
    "- Accuracy for train: 0.89\n",
    "- Some signs of overfitting, but not as bad as the default LinearSVM Classifier from Step 3\n",
    "\n",
    "Model 5 - SVM + Polynomial Kernel\n",
    "\n",
    "- Best parameters: SVC(C=4.21, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "- Accuracy for test: 0.76\n",
    "- Accuracy for train: 0.98\n",
    "- Obvious signs of overfitting\n",
    "\n",
    "Model 6 - SVM + RBF Kernel\n",
    "\n",
    "- Best parameters: SVC(C=4.21, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "- Accuracy for test: 0.83\n",
    "- Accuracy for train: 0.99\n",
    "- Obvious signs of overfitting\n",
    "\n",
    "The linear, polynomial and RBF or Gaussian kernel are different approaches to making the hyperplane decision boundary between the classes where the kernel functions are used to map the original dataset (linear/nonlinear ) into a higher dimensional space with view to making it linear dataset. Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more features, such as polynomial features. Usually linear and polynomial kernels are less time consuming and provides less accuracy than the rbf or Gaussian kernels.\n",
    "\n",
    "Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms, but at a low polynomial degree it cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow. Fortunately, when using SVMs we can apply a mathematical technique called the kernel trick where there is no combinatorial explosion of the number of features since you dont actually add any features.\n",
    "\n",
    "As for RBF, just like the polynomial features method, the similarity features method can be useful with any Machine Learning algorithm, but it may be computationally expensive to compute all the additional features, especially on large training sets. However, once again we can use kernel trick where it makes it possible to obtain a similar result as if we had added many similarity features, without actually having to add them.\n",
    "\n",
    "To summarize, Linear SVM is a parametric model, an RBF kernel SVM isn't, and the complexity of the latter grows with the size of the training set. Not only is it more expensive to train an RBF kernel SVM, but you also have to keep the kernel matrix around, and the projection into this \"infinite\" higher dimensional space where the data becomes linearly separable is more expensive as well during prediction. Furthermore, you have more hyperparameters to tune, so model selection is more expensive as well. And finally, it's much easier to overfit a complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Explain the 'C' parameter used in steps 4-6. What does a small C mean versus a large C in sklearn? Why is it important to use the 'C' parameter when fitting a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally in a SVM we are searching for two things: a hyperplane with the largest minimum margin, and a hyperplane that correctly separates as many instances as possible. The problem is that we will not always be able to get both things. The c parameter determines how great our desire is for the latter.\n",
    "\n",
    "The C parameter (essentially a regularisation parameter, which controls the trade-off between achieving a low error on the training data and minimizing the norm of the weights.) tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the classifier makes fewer margin violations but ends up with a smaller margin basically the model will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C leads to a wider boundaries but more margin violations basically it will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Scaling our input data does not matter much for Random Forest, but it is a critical step for Support Vector Machines. Explain why this is such a critical step. Also, provide an example of a feature from this data set that could cause issues with our SVMs if not scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main purpose of scaling data before processing is to avoid attributes in greater numeric ranges. Other purpose is to avoid some types of numerical difficulties during calculation. Large attribute values might cause numerical problems. Scaling is very important in case of variables with large variances. To put it differently, SVM tries to maximize the distance between the separating plane and the support vectors. If one feature has very large values, it will dominate the other features when calculating the distance. So overall, scaling will help SVM's performance to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BrdIndx</th>\n",
       "      <th>Area</th>\n",
       "      <th>Round</th>\n",
       "      <th>Bright</th>\n",
       "      <th>Compact</th>\n",
       "      <th>ShpIndx</th>\n",
       "      <th>Mean_G</th>\n",
       "      <th>Mean_R</th>\n",
       "      <th>Mean_NIR</th>\n",
       "      <th>SD_G</th>\n",
       "      <th>...</th>\n",
       "      <th>SD_NIR_140</th>\n",
       "      <th>LW_140</th>\n",
       "      <th>GLCM1_140</th>\n",
       "      <th>Rect_140</th>\n",
       "      <th>GLCM2_140</th>\n",
       "      <th>Dens_140</th>\n",
       "      <th>Assym_140</th>\n",
       "      <th>NDVI_140</th>\n",
       "      <th>BordLngth_140</th>\n",
       "      <th>GLCM3_140</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>507.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.025720</td>\n",
       "      <td>562.504931</td>\n",
       "      <td>1.237574</td>\n",
       "      <td>165.612939</td>\n",
       "      <td>2.187081</td>\n",
       "      <td>2.277318</td>\n",
       "      <td>166.290355</td>\n",
       "      <td>162.291953</td>\n",
       "      <td>168.256667</td>\n",
       "      <td>10.725227</td>\n",
       "      <td>...</td>\n",
       "      <td>24.601144</td>\n",
       "      <td>2.931657</td>\n",
       "      <td>0.817712</td>\n",
       "      <td>0.597732</td>\n",
       "      <td>8.048698</td>\n",
       "      <td>1.455838</td>\n",
       "      <td>0.653905</td>\n",
       "      <td>0.027436</td>\n",
       "      <td>1398.706114</td>\n",
       "      <td>1101.998185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.619254</td>\n",
       "      <td>698.655240</td>\n",
       "      <td>0.561988</td>\n",
       "      <td>63.230806</td>\n",
       "      <td>0.874054</td>\n",
       "      <td>0.718441</td>\n",
       "      <td>59.217648</td>\n",
       "      <td>73.455101</td>\n",
       "      <td>69.702475</td>\n",
       "      <td>4.968761</td>\n",
       "      <td>...</td>\n",
       "      <td>12.203441</td>\n",
       "      <td>4.942887</td>\n",
       "      <td>0.106007</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.787912</td>\n",
       "      <td>0.451781</td>\n",
       "      <td>0.251287</td>\n",
       "      <td>0.133834</td>\n",
       "      <td>1097.323462</td>\n",
       "      <td>533.927869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.850000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>22.910000</td>\n",
       "      <td>26.520000</td>\n",
       "      <td>31.110000</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>-0.360000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>211.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.580000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>127.485000</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.715000</td>\n",
       "      <td>146.460000</td>\n",
       "      <td>97.585000</td>\n",
       "      <td>111.715000</td>\n",
       "      <td>6.985000</td>\n",
       "      <td>...</td>\n",
       "      <td>14.485000</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>7.370000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>726.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.950000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>170.650000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.180000</td>\n",
       "      <td>189.630000</td>\n",
       "      <td>158.280000</td>\n",
       "      <td>167.750000</td>\n",
       "      <td>9.290000</td>\n",
       "      <td>...</td>\n",
       "      <td>22.380000</td>\n",
       "      <td>1.920000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1011.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.380000</td>\n",
       "      <td>681.500000</td>\n",
       "      <td>1.565000</td>\n",
       "      <td>224.825000</td>\n",
       "      <td>2.490000</td>\n",
       "      <td>2.675000</td>\n",
       "      <td>206.780000</td>\n",
       "      <td>237.375000</td>\n",
       "      <td>238.480000</td>\n",
       "      <td>13.330000</td>\n",
       "      <td>...</td>\n",
       "      <td>33.825000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>1.775000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>1874.000000</td>\n",
       "      <td>1335.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.530000</td>\n",
       "      <td>5767.000000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>245.870000</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>5.410000</td>\n",
       "      <td>239.370000</td>\n",
       "      <td>253.610000</td>\n",
       "      <td>253.630000</td>\n",
       "      <td>30.870000</td>\n",
       "      <td>...</td>\n",
       "      <td>61.340000</td>\n",
       "      <td>64.700000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.570000</td>\n",
       "      <td>2.410000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>8896.000000</td>\n",
       "      <td>3619.280000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          BrdIndx         Area       Round      Bright     Compact  \\\n",
       "count  507.000000   507.000000  507.000000  507.000000  507.000000   \n",
       "mean     2.025720   562.504931    1.237574  165.612939    2.187081   \n",
       "std      0.619254   698.655240    0.561988   63.230806    0.874054   \n",
       "min      1.000000    22.000000    0.000000   26.850000    1.000000   \n",
       "25%      1.580000   159.000000    0.840000  127.485000    1.650000   \n",
       "50%      1.950000   323.000000    1.210000  170.650000    2.000000   \n",
       "75%      2.380000   681.500000    1.565000  224.825000    2.490000   \n",
       "max      4.530000  5767.000000    3.520000  245.870000    8.070000   \n",
       "\n",
       "          ShpIndx      Mean_G      Mean_R    Mean_NIR        SD_G  ...  \\\n",
       "count  507.000000  507.000000  507.000000  507.000000  507.000000  ...   \n",
       "mean     2.277318  166.290355  162.291953  168.256667   10.725227  ...   \n",
       "std      0.718441   59.217648   73.455101   69.702475    4.968761  ...   \n",
       "min      1.040000   22.910000   26.520000   31.110000    3.550000  ...   \n",
       "25%      1.715000  146.460000   97.585000  111.715000    6.985000  ...   \n",
       "50%      2.180000  189.630000  158.280000  167.750000    9.290000  ...   \n",
       "75%      2.675000  206.780000  237.375000  238.480000   13.330000  ...   \n",
       "max      5.410000  239.370000  253.610000  253.630000   30.870000  ...   \n",
       "\n",
       "       SD_NIR_140      LW_140   GLCM1_140    Rect_140   GLCM2_140    Dens_140  \\\n",
       "count  507.000000  507.000000  507.000000  507.000000  507.000000  507.000000   \n",
       "mean    24.601144    2.931657    0.817712    0.597732    8.048698    1.455838   \n",
       "std     12.203441    4.942887    0.106007    0.197505    0.787912    0.451781   \n",
       "min      2.650000    1.000000    0.200000    0.100000    5.690000    0.240000   \n",
       "25%     14.485000    1.375000    0.770000    0.455000    7.370000    1.160000   \n",
       "50%     22.380000    1.920000    0.840000    0.610000    8.020000    1.440000   \n",
       "75%     33.825000    2.800000    0.890000    0.760000    8.750000    1.775000   \n",
       "max     61.340000   64.700000    0.970000    1.000000    9.570000    2.410000   \n",
       "\n",
       "        Assym_140    NDVI_140  BordLngth_140    GLCM3_140  \n",
       "count  507.000000  507.000000     507.000000   507.000000  \n",
       "mean     0.653905    0.027436    1398.706114  1101.998185  \n",
       "std      0.251287    0.133834    1097.323462   533.927869  \n",
       "min      0.030000   -0.360000      34.000000   211.270000  \n",
       "25%      0.470000   -0.080000     601.000000   726.745000  \n",
       "50%      0.710000   -0.020000    1148.000000  1011.230000  \n",
       "75%      0.860000    0.145000    1874.000000  1335.640000  \n",
       "max      1.000000    0.370000    8896.000000  3619.280000  \n",
       "\n",
       "[8 rows x 147 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example would be \"BordLngth_140\", with the mean of 1398.71 and std of 1097.32. The values and range for \"BordLngth_140\" is very different from some features such as \"BrdIndx\", with the mean of 2.03 and std of 0.62."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Describe conceptually what the purpose of a kernel is for Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, a kernel is usually used to refer to the kernel trick, a method of using a linear classifier to solve a non-linear problem. It entails transforming linearly inseparable data like to linearly separable ones. The kernel function is what is applied on each data instance to map the original non-linear observations into a higher-dimensional space in which they become separable. For exmaple, SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types i.e. linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. To summarize, if we want transform our existing data into a higher dimensional data, which in many cases help us classify better we need not compute the exact transformation of our data, we just need the inner product of our data in that higher dimensional space. This works in datasets which arent linearly separable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
